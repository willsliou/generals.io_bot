\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Milestone 1}
\author{Wills Liou}
\date{\today}
\begin{document}

\maketitle


\section{Milestone 1} 
Goal: Understand an advanced data structure or algorithm to be used in Generals, including analysis and implementation details \\

Two Ideas: (MCTS implementation was inspired by the search algorithm used by Deep Mind's Alpha Zero AI) \\ 
\begin{enumerate}
\item Use Monte Carlo Tree Search with a neural network (reinforcement learning). Discover states on the go as opposed to DP. \\
Search every possible move that exists after each turn and select the node with highest probability of winning. Our win probability is calculated by comparing how close the output of the child node's decision was to final output needed to win the game; we reward the best child node. \\ \\
Use \textbf{Bellman equation} to estimate returns from each state for given policy (must use expected values as our information is imperfect) \\
Bellman Equation \\
$V(s) = max_a(R(s,a) + \gamma V(s'))$ where $Q(s,a) = R(s,a) + \gamma V(s'))$ for in state $S$ we estimate Q for every possible action. \\ 
Then we choose action with the highest Q value. \textbf{This function is looking for the return value we get from getting the best action.} \\

Policy function gets the best action for every state \\ 
$\pi(s) = max_a(Q(s,a))$ reads as \textit{the policy for state $s$ is to choose the action with the highest Q value}. \\ This is policy function makes the same calculations as the Bellman equation, but instead is returning what the best action is.

\textbf{Use Q Learning} \\
Assume return is $G$. 
Our formula for returns we received at step $t$ of the game. $G_t = r_{(t+1)} + \gamma G_{(t+1)}$ \\
This reads as $G_t$ equals the immediate return received plus all discounted rewards thereafter received after following our current policy.

\textbf{Algorithm to calculate returns after each episode}
\begin{enumerate}
\item Inititalize G = 0
\item empty list statesReturns = []
\item Loop backwards through list of statesReturns after playing game
\item append (s,G) to statesReturns list
\item $G = r + \gamma \times G$
\item reverse back states. Returns to original order
\end{enumerate}

\end{enumerate}


\textbf{Idea: Use Episilon greedy algorithm have balanced results} \\

\textbf{Improve runtime}
\begin{enumerate}
\item Don't visit same state more than once
\item take the first visit and discard all other visits
\end{enumerate}

\textbf{Steps for Reinforcement Learning}
\begin{enumerate}
\item Selection
\begin{enumerate}
\item Start at root node
\item If left node (L) is more optimal or has a larger probability of winning, select left node; if this left node is a leaf, end the game.
\end{enumerate}


\item Expand
\begin{enumerate}
\item Calculate, create child nodes for each action based on our selected nodes, select the first one of these nodes
\item Use confidence bound (upper) to select  $v_i$ + $C \cdot \sqrt{\frac{ln N}{n_i}}$ where C is some constant, $v_i$ is value of state at $i$, $n_i$ is number of visits to state i, $N$ is number of visits on the same level
\item Markov Decision process - maximize the rewards
\end{enumerate}

\item Simulation ($S_i$)
\begin{enumerate}
\item Take random actions, and retrieve state. (repeat until at terminal node and return that terminal node's value)
\item Use reinforcement learning to reward children nodes that 
\end{enumerate}

\item Backprogation (backwards propagation of errors) using gradient descent
\begin{enumerate}
\item Reduce error by analyzing and fine tuning our weights based on error rate in previous epoch or transition
\end{enumerate}
\end{enumerate}
Some downsides are that it might need millions of examples, solve by fetching public replays

Idea 2: Use dynamic programming - brute force and loop through all possible states and actions 
\begin{enumerate}
\item Consider n-tiles around yourself.
\item Assume the worst possible scenarios for yourself (minimum gain by doing any move)
\item Store those in the dp table
\item Calculate back to your current state
\item You can write it like you are maximizing your minimum gain that chooses the strategy to have the highest minimum gain.
\end{enumerate}
Some downsides are that it might be too slow, usually unpractical for large state spaces.

Other algorithms:
A simulating annealing algorithm is probably not a good choice here (overlap heavly with monte carlo) compared to our other options.
Exploring - visit more states to climb further
Exploiting - hill climbing is always exploiting, can lead to getting stuck
How SA algorithm works
\begin{enumerate}
\item Assign high initial value and slowly decrease it (similar to cooling it in the annealing process)
\item Slowly decrease the acceptance of worse solutions as our temperature decreases

Genetic algorithms -  Snapshots or states of the game can be represented by ${(s0,a0),s1,a1),...,(sN,aN)}$
Minimax - Doesn't scale well. Needs to evaluate thoughly with alpha beta etc.  \\
MCTS is better because only searches several layers into the tree, prioritizes which part of trees; simulates outcome instead of exhaustively expanding search spaces and limit how many evaluations to make. Limits the branching factor (number of times you branch down tree; possibilities)


\end{document}